{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Read and Convert PDF to Audio",
      "provenance": [],
      "collapsed_sections": [
        "sDvJULe_2fCD"
      ],
      "authorship_tag": "ABX9TyOaximzhBpUQL0tOWZU2r/A"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jWeexOI1l0cg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29898801-93fa-46ed-c663-54c4fdd1b0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-36.0.2 pdfminer.six-20211012\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "VrFCXuPWnEsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de50fde-248a-4e66-8aad-9f43f10e0661"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-1.4.2-py3-none-any.whl (11 kB)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=fb36c7fb211ab448adb01d516bcedf9b07bb7318103cb0a823e3ae091f51b4dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-1.4.2 feedparser-6.0.8 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "8_AGaiW_xY-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a612b6c-9a54-4c71-fc0e-8bcdee3105e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.2.4-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gTTS) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gTTS) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from gTTS) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (3.0.4)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyttsx3"
      ],
      "metadata": {
        "id": "mwLhYQyBYgwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac1722f-e227-4a1e-8a1e-c3dec490b4df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyttsx3\n",
            "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: pyttsx3\n",
            "Successfully installed pyttsx3-2.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install espeak"
      ],
      "metadata": {
        "id": "2-FuQTpiY5rp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802300b1-d4de-47ec-87a6-d5817dc0ca14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,219 kB of archives.\n",
            "After this operation, 3,031 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 1,219 kB in 1s (915 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 155335 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v4i8AdNa73G",
        "outputId": "f2b9c2c4-4451-4914-ed65-687d02e1c508"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfminer\n",
        "from pdfminer.high_level import extract_text, extract_pages\n",
        "from pdfminer.layout import LTTextContainer"
      ],
      "metadata": {
        "id": "JtywpHmtrEXG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv"
      ],
      "metadata": {
        "id": "XmcjCVq72HYC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS"
      ],
      "metadata": {
        "id": "1tXGcoEBw2RQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "sDvJULe_2fCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = arxiv.Search(id_list=[\"2203.09207\"])\n",
        "paper = next(search.results())\n",
        "print(paper.title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANJ3iBCSpA8E",
        "outputId": "e8246543-0f3e-4cd7-a894-02eb4f51fff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper = next(arxiv.Search(id_list=[\"2203.09207\"]).results())\n",
        "# Download the PDF to the PWD with a default filename.\n",
        "paper.download_pdf(filename=(paper.title+\".pdf\"))"
      ],
      "metadata": {
        "id": "1RlXQxllpxOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text(paper.title+'.pdf')"
      ],
      "metadata": {
        "id": "M-_mx-WWp72f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "o6g_4R4usI_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tts = gTTS(text)\n",
        "tts.save(paper.title+\".mp3\")"
      ],
      "metadata": {
        "id": "UFi0ot5CxfOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textract"
      ],
      "metadata": {
        "id": "-zyByHK31Zi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text = textract.process(paper.title+'.pdf', method='pdfminer', encoding='utf-8')\n",
        "#text = text.encode(\"utf-8\")\n",
        "text"
      ],
      "metadata": {
        "id": "P-9w7zXt2De1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dec = text.decode('utf-8')\n",
        "print(text_dec)"
      ],
      "metadata": {
        "id": "Gve73JHp51k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = text_dec.splitlines()\n",
        "test"
      ],
      "metadata": {
        "id": "Wvox1zgT6I1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = []\n",
        "final_text=\"\"\n",
        "for i in test:\n",
        "  if len(i)>1:\n",
        "    final.append(i)\n",
        "    final_text+= \" \"+i\n",
        "\n",
        "final"
      ],
      "metadata": {
        "id": "AC7v76oQ3HpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_text"
      ],
      "metadata": {
        "id": "oEClm9pl7gB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tts = gTTS(final_text)\n",
        "tts.save(paper.title+\".mp3\")"
      ],
      "metadata": {
        "id": "n037ec0f8LHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(list(extract_pages(paper.title+'.pdf'))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iUDvYBpOluh",
        "outputId": "72c3471f-62d0-4b83-8605-f60ecb115ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page = extract_pages('/content/GAN You Do the GAN GAN?.pdf', page_numbers=[0])\n",
        "page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMppoqhyOnaS",
        "outputId": "975c0a7a-5e04-407d-ae67-992462e19a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object extract_pages at 0x7f8a64b24ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for page_layout in page:\n",
        "    for element in page_layout:\n",
        "        if isinstance(element, LTTextContainer):\n",
        "            print(element.get_text())"
      ],
      "metadata": {
        "id": "S8WyNeptRN3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#APIs"
      ],
      "metadata": {
        "id": "UhkPskqt2bN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query=\"\", max_results=10, sort_by=\"Relevance\", sort_order=\"Descending\"):\n",
        "  \n",
        "  sr_by_dict = {\"Relevance\": arxiv.SortCriterion.Relevance, \"Last Updated Date\": arxiv.SortCriterion.LastUpdatedDate, \"Submitted Date\": arxiv.SortCriterion.SubmittedDate}\n",
        "  sr_or_dict = {\"Descending\": arxiv.SortOrder.Descending, \"Ascending\": arxiv.SortOrder.Ascending}\n",
        "    \n",
        "  search = arxiv.Search(\n",
        "                        query = query,\n",
        "                        max_results = max_results,\n",
        "                        sort_by = sr_by_dict[sort_by],\n",
        "                        sort_order =  sr_or_dict[sort_order])\n",
        "\n",
        "  #for result in search.results():\n",
        "    #print(result.title)\n",
        "\n",
        "  return search"
      ],
      "metadata": {
        "id": "4BpoeJP3Pyr-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = search(query = \"gans\", max_results=10, sort_by=\"Relevance\", sort_order=\"Descending\")\n",
        "for i in search.results():\n",
        "  id = i.entry_id.split(\"/\")\n",
        "  print(i.title+\" - \"+ str(id[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMCmoluE3uJV",
        "outputId": "b75552db-dce2-4706-b222-22e6e5230672"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey - 2111.13282v1\n",
            "GAN You Do the GAN GAN? - 1904.00724v1\n",
            "Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities - 1701.06264v6\n",
            "Capsule GAN Using Capsule Network for Generator Architecture - 2003.08047v1\n",
            "Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder - 2002.02112v1\n",
            "From GAN to WGAN - 1904.08994v1\n",
            "Monte Carlo Simulation of SDEs using GANs - 2104.01437v1\n",
            "\"WM\"-Shaped Growth of GaN on Patterned Sapphire Substrates - 1611.08337v1\n",
            "Unbiased Auxiliary Classifier GANs with MINE - 2006.07567v1\n",
            "GAN Inversion: A Survey - 2101.05278v4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paper(paper = \"GAN You Do the GAN GAN? - 1904.00724v1\"):\n",
        "  id = paper.split(\" - \")\n",
        "  paper = next(arxiv.Search(id_list=[id[-1]]).results())\n",
        "  paper.download_pdf(filename=(paper.title+\".pdf\"))\n",
        "\n",
        "  return(paper)"
      ],
      "metadata": {
        "id": "BXS1m0hS4wq8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper = get_paper('GAN You Do the GAN GAN? - 1904.00724v1')\n",
        "paper.title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6LGt2N1oBQ70",
        "outputId": "9db97419-0d46-40f2-b162-e1ebcde9ce29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GAN You Do the GAN GAN?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_total_pages(paper):\n",
        "  return(len(list(extract_pages(paper.title+'.pdf'))))"
      ],
      "metadata": {
        "id": "nFz6Vo9aBVX6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_page = get_total_pages(paper)\n",
        "total_page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBsFNRY1DIIC",
        "outputId": "809bf255-f271-4fa0-deeb-263651e4dd3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pages(paper, start_page=0, end_page=total_page):\n",
        "  page_number = []\n",
        "  for i in range(start_page, end_page):\n",
        "    page_number.append(i)\n",
        "  print(page_number)\n",
        "  filename =str(paper.title)+'.pdf'\n",
        "  page = extract_pages(filename, page_numbers=page_number)\n",
        "\n",
        "  return page"
      ],
      "metadata": {
        "id": "yPCxwXi3DNJh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = get_pages(paper)\n",
        "content = \"\"\n",
        "for page_layout in pages:\n",
        "    for element in page_layout:\n",
        "        if isinstance(element, LTTextContainer):\n",
        "            content = content+element.get_text()"
      ],
      "metadata": {
        "id": "TJ6aX4F0HEIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656957d1-7d5f-429e-c390-3879e9e1a415"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ZCrjv9NgU5lC",
        "outputId": "a65d0b71-351c-45da-f9e9-e55ed165503f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GAN You Do the GAN GAN?\\nJoseph Suarez\\n9\\n1\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n4\\n2\\n7\\n0\\n0\\n.\\n4\\n0\\n9\\n1\\n:\\nv\\ni\\nX\\nr\\na\\nAbstract\\nGenerative Adversarial Networks (GANs) have\\nbecome a dominant class of generative models.\\nIn recent years, GAN variants have yielded es-\\npecially impressive results in the synthesis of a\\nvariety of forms of data. Examples include com-\\npelling natural and artistic images, textures, mu-\\nsical sequences, and 3D object ﬁles. However,\\none obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning’s most\\npressing questions: GAN you do the GAN GAN?\\nThat is, is it possible to train a GAN to model a\\ndistribution of GANs? We release the full source\\ncode for this project under the MIT license.1\\n1. Introduction, Background, Related Work\\nGANs (Goodfellow et al., 2014) have become perhaps the\\nsingle most prevalent class of generative models in recent\\nyears, spawning hundreds of variants and an entire subﬁeld\\nof surrounding work. While they are notoriously difﬁcult to\\ntrain and often suffer from mode collapse, under the right\\nconditions, GANs have been shown to generate compelling\\nartiﬁcial data distributions and are perhaps best known for\\nrealistic image synthesis (Zhu et al., 2017).\\nOur objective is to apply GANs to a different class of data:\\nGANs themselves. Instead of using a GAN to model images,\\nwe use a GAN to model a distribution of GANs that model\\nimages. We refer to this architecture over architectures\\nas a GAN-GAN. We do not consider GAN-GAN-GANs\\nor GAN-GAN-GAN-GANs in this work. While these are\\nstraightforward to implement, they would require an ex-\\nponential parameter budget (at least as formulated in the\\npresent work) which we lack the hardware to support.\\nHypernetworks (Ha et al., 2016) and extensions thereof\\n(Deutsch, 2018; Suarez, 2017) have also explored the con-\\ncept of using one network to generate the weights of another.\\nHowever, to our knowledge, all such settings typically learn\\nboth the architecture and the meta-architecture end-to-end.\\nIn contrast, we are interested in whether it is possible to\\ndirectly learn a distribution over architectures within the\\n1Full source code:\\nhttps://github.com/jsuarez5341/gan-you-do-the-gan-gan\\nFigure 1. GAN-GAN: a GAN trained on a dataset of GANs\\nstandard setting of generative modeling. We treat GANs\\nthemselves as examples and learn a GAN-GAN by training\\non small set of trained GANs.\\nThis paper is a joke; however, the results are in fact real.\\nInterpretability is a key problem in deep learning, especially\\nin models to be deployed in real world systems. Generative\\nmodeling over networks could serve as a useful tool for\\nvisualization and analysis of network decisions and results.\\n2. Methods\\nGANs formulate a two player game between networks. For-\\nmally, GANs deﬁnes a Generator G and a Discriminator\\nD. The Discriminator models the probability P (G|x) that\\na given example x is fake. The Generator maximizes the\\nprobability P (D(G(z))) (z is sampled noise) that the Dis-\\ncriminator will output an incorrect prediction. A GAN-GAN\\nis simply a GAN trained on a dataset of GAN weights.\\nAlgorithm 1 GAN-GAN Training. We ﬁrst train a set of\\nGANs and save snapshots of the parameters each epoch. We\\nthen train a GAN-GAN (a GAN over GANS) by treating\\neach snapshot as an individual training example.\\nfor GAN Index = 1...#Networks do\\nInitialize an MNIST GAN\\nfor Epoch = 1...#Epochs do\\nTrain the MNIST GAN for one epoch\\nSave a snapshot of the GAN parameters\\nend for\\nend for\\nLoad all snapshots of all GANs into a dataset with #Net-\\nworks × #Epochs examples\\nTrain a GAN over the dataset of GAN snapshots\\nGAN-GAN\\nFigure 2. Example samples from the training of an MNIST GAN (top-bottom left-right: epochs 1, 2, 10, 25, 27, 30, 32, 35, 40, 49)\\nFigure 3. Image samples from GANs sampled from the trained GAN-GAN. Rows correspond to GANs linearly sampled from 1D\\nGAN-GAN latent space in the interval (-2, 2). Columns correspond to a particular noise vector input to all GANs.\\nGAN-GAN\\nwere able to obtain signiﬁcantly better image samples using\\na slightly modiﬁed DCGAN with under 30k parameters, we\\nfound it more difﬁcult for the GAN-GAN to learn to model\\nDCGAN with very small amounts of data. This is expected,\\nas small perturbations to the weights in the ﬁrst layers of\\nthe signiﬁcantly deeper DCGAN model can signiﬁcantly\\nalter the output of the ﬁnal layer. While it is likely that our\\napproach would be viable with a larger dataset of DCGAN\\nsnapshots, this work was conducted with limited personal\\nhardware. As such, we opted to use a small fully connected\\nmodel and were unable to test GAN-GANs over DCGAN\\nvariants in detail.\\n4. Conclusion\\nWe train a GAN over training snapshots of small, fully\\nconnected MNIST GANs and demonstrate that generative\\nmodels are capable of learning useful representations of\\nother generative models. In short: GAN you do the GAN\\nGAN? Yes you GAN!\\nReferences\\nDeutsch, L. Generating Neural Networks with Neural Networks.\\narXiv e-prints, art. arXiv:1801.01952, Jan 2018.\\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-\\nFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative\\nAdversarial Networks. arXiv e-prints, art. arXiv:1406.2661, Jun\\n2014.\\nHa, D., Dai, A., and Le, Q. V. HyperNetworks. arXiv e-prints, art.\\narXiv:1609.09106, Sep 2016.\\nKingma, D. P. and Ba, J. Adam: A Method for Stochastic Opti-\\nmization. arXiv e-prints, art. arXiv:1412.6980, Dec 2014.\\nLiu, J., Tripathi, S., Kurup, U., and Shah, M. Make (Nearly)\\nEvery Neural Network Better: Generating Neural Network En-\\nsembles by Weight Parameter Resampling. arXiv e-prints, art.\\narXiv:1807.00847, Jul 2018.\\nRadford, A., Metz, L., and Chintala, S. Unsupervised Representa-\\ntion Learning with Deep Convolutional Generative Adversarial\\nNetworks. arXiv e-prints, art. arXiv:1511.06434, Nov 2015.\\n3. Experiments and Discussion\\nGAN Architecture The generator and discriminator are\\nthree layer (input-hidden-output) fully connected neural net-\\nworks with hidden dimension 64. We use Leaky ReLU(Xu\\net al., 2015) activations with 0.2 negative slope after the\\ninput and hidden layers. We use tanh for the generator out-\\nput (dimensionality 28 × 28 = 784 to match MNIST) and\\nsigmoid for the discriminator output (dimensionality 1). The\\ngenerator samples from latent dimension 64.\\nGAN-GAN Architecture The GAN-GAN generator and\\ndiscriminator have the same layer and activation structure\\nas the MNIST GAN. The input dimensionality is 113745,\\nwhich is equal to the dimensionality of the GAN parameter\\nvector. We found that using a smaller hidden dimension\\nfor the discriminator (8) than the generator (64) helped to\\nstabilize training. We use latent dimension 1 for the GAN-\\nGAN in order to enable visualizations. Results improve\\nwith a larger latent space.\\nTraining We use Adam(Kingma & Ba, 2014) for all net-\\nworks. The learning rate is ﬁxed to 0.0002; all other pa-\\nrameters are PyTorch defaults. We use batch sizes 128 and\\n32 for MNIST and the GAN-GAN, respectively. As de-\\nscribed in Algorithm 1, we train 35 MNIST GANs for 100\\nepochs each, saving snapshots of the weights at each epoch.\\nWe train the GAN-GAN for 250 epochs using these 3500\\nsnapshots as training examples.\\nResults\\nIn order to evaluate the performance of the GAN-\\nGAN, we ﬁrst linearly sample 32 GANs from the 1-\\ndimensional latent space of the GAN-GAN. We then ﬁx\\n40 noise samples with the same dimensionality as the GAN\\nlatent space. Finally, we sample 40 images from each GAN\\nusing the ﬁxed noise samples. Fig. 3 shows the results. The\\nGAN-GAN produces neatly ordered GAN samples accord-\\ning to image quality. Surprisingly, the GAN-GAN actually\\nexhibits better performance than the trained GANs. Given\\nthat high quality GANs appear more real than low quality\\nGANs, it is possible that the GAN-GAN learned to bootstrap\\nits own performance by implicitly combining snapshots (Liu\\net al., 2018) in latent space, in effect producing a smoothed\\nhistory over training.\\nDiscussion\\nWhile the GAN-GAN samples are of high quality with\\nrespect to the MNIST GAN samples, the MNIST GAN sam-\\nples themselves are of fairly low quality (Fig. 2). As the\\nweight dimensionality of each GAN is equal to the input\\ndimensionality of the GAN-GAN, we were constrained to\\nuse a very small network for the MNIST GAN. One obvious\\nsolution is to use a more parameter efﬁcient network archi-\\ntecture such as DCGAN (Radford et al., 2015). While we\\nSuarez,\\nIn Guyon,\\nLanguage modeling with recurrent highway\\nJ.\\nhypernetworks.\\nI., Luxburg, U. V., Ben-\\ngio, S., Wallach, H., Fergus, R., Vishwanathan, S., and\\nGarnett, R. (eds.), Advances in Neural Information Pro-\\ncessing Systems 30, pp. 3267–3276. Curran Associates,\\nInc., 2017. URL http://papers.nips.cc/paper/\\n6919-language-modeling-with-recurrent-highway-hypernetworks.\\npdf.\\nXu, B., Wang, N., Chen, T., and Li, M. Empirical Evaluation of\\nRectiﬁed Activations in Convolutional Network. arXiv e-prints,\\nart. arXiv:1505.00853, May 2015.\\nZhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired Image-\\nto-Image Translation using Cycle-Consistent Adversarial Net-\\nworks. arXiv e-prints, art. arXiv:1703.10593, Mar 2017.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(content=content):\n",
        "  text = content.splitlines()\n",
        "  final_text=\"\"\n",
        "  for i in text:\n",
        "    if len(i)>1:\n",
        "      final_text+= \" \"+i\n",
        "\n",
        "  return final_text"
      ],
      "metadata": {
        "id": "8A8fJ3lYHRD2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = pre_process()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "aCuUVNuWVbE3",
        "outputId": "13a55ca7-dcf9-4806-bbd4-3dc1bdcb05fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' GAN You Do the GAN GAN? Joseph Suarez Abstract Generative Adversarial Networks (GANs) have become a dominant class of generative models. In recent years, GAN variants have yielded es- pecially impressive results in the synthesis of a variety of forms of data. Examples include com- pelling natural and artistic images, textures, mu- sical sequences, and 3D object ﬁles. However, one obvious synthesis candidate is missing. In this work, we answer one of deep learning’s most pressing questions: GAN you do the GAN GAN? That is, is it possible to train a GAN to model a distribution of GANs? We release the full source code for this project under the MIT license.1 1. Introduction, Background, Related Work GANs (Goodfellow et al., 2014) have become perhaps the single most prevalent class of generative models in recent years, spawning hundreds of variants and an entire subﬁeld of surrounding work. While they are notoriously difﬁcult to train and often suffer from mode collapse, under the right conditions, GANs have been shown to generate compelling artiﬁcial data distributions and are perhaps best known for realistic image synthesis (Zhu et al., 2017). Our objective is to apply GANs to a different class of data: GANs themselves. Instead of using a GAN to model images, we use a GAN to model a distribution of GANs that model images. We refer to this architecture over architectures as a GAN-GAN. We do not consider GAN-GAN-GANs or GAN-GAN-GAN-GANs in this work. While these are straightforward to implement, they would require an ex- ponential parameter budget (at least as formulated in the present work) which we lack the hardware to support. Hypernetworks (Ha et al., 2016) and extensions thereof (Deutsch, 2018; Suarez, 2017) have also explored the con- cept of using one network to generate the weights of another. However, to our knowledge, all such settings typically learn both the architecture and the meta-architecture end-to-end. In contrast, we are interested in whether it is possible to directly learn a distribution over architectures within the 1Full source code: https://github.com/jsuarez5341/gan-you-do-the-gan-gan Figure 1. GAN-GAN: a GAN trained on a dataset of GANs standard setting of generative modeling. We treat GANs themselves as examples and learn a GAN-GAN by training on small set of trained GANs. This paper is a joke; however, the results are in fact real. Interpretability is a key problem in deep learning, especially in models to be deployed in real world systems. Generative modeling over networks could serve as a useful tool for visualization and analysis of network decisions and results. 2. Methods GANs formulate a two player game between networks. For- mally, GANs deﬁnes a Generator G and a Discriminator D. The Discriminator models the probability P (G|x) that a given example x is fake. The Generator maximizes the probability P (D(G(z))) (z is sampled noise) that the Dis- criminator will output an incorrect prediction. A GAN-GAN is simply a GAN trained on a dataset of GAN weights. Algorithm 1 GAN-GAN Training. We ﬁrst train a set of GANs and save snapshots of the parameters each epoch. We then train a GAN-GAN (a GAN over GANS) by treating each snapshot as an individual training example. for GAN Index = 1...#Networks do Initialize an MNIST GAN for Epoch = 1...#Epochs do Train the MNIST GAN for one epoch Save a snapshot of the GAN parameters end for end for Load all snapshots of all GANs into a dataset with #Net- works × #Epochs examples Train a GAN over the dataset of GAN snapshots GAN-GAN Figure 2. Example samples from the training of an MNIST GAN (top-bottom left-right: epochs 1, 2, 10, 25, 27, 30, 32, 35, 40, 49) Figure 3. Image samples from GANs sampled from the trained GAN-GAN. Rows correspond to GANs linearly sampled from 1D GAN-GAN latent space in the interval (-2, 2). Columns correspond to a particular noise vector input to all GANs. GAN-GAN were able to obtain signiﬁcantly better image samples using a slightly modiﬁed DCGAN with under 30k parameters, we found it more difﬁcult for the GAN-GAN to learn to model DCGAN with very small amounts of data. This is expected, as small perturbations to the weights in the ﬁrst layers of the signiﬁcantly deeper DCGAN model can signiﬁcantly alter the output of the ﬁnal layer. While it is likely that our approach would be viable with a larger dataset of DCGAN snapshots, this work was conducted with limited personal hardware. As such, we opted to use a small fully connected model and were unable to test GAN-GANs over DCGAN variants in detail. 4. Conclusion We train a GAN over training snapshots of small, fully connected MNIST GANs and demonstrate that generative models are capable of learning useful representations of other generative models. In short: GAN you do the GAN GAN? Yes you GAN! References Deutsch, L. Generating Neural Networks with Neural Networks. arXiv e-prints, art. arXiv:1801.01952, Jan 2018. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative Adversarial Networks. arXiv e-prints, art. arXiv:1406.2661, Jun 2014. Ha, D., Dai, A., and Le, Q. V. HyperNetworks. arXiv e-prints, art. arXiv:1609.09106, Sep 2016. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Opti- mization. arXiv e-prints, art. arXiv:1412.6980, Dec 2014. Liu, J., Tripathi, S., Kurup, U., and Shah, M. Make (Nearly) Every Neural Network Better: Generating Neural Network En- sembles by Weight Parameter Resampling. arXiv e-prints, art. arXiv:1807.00847, Jul 2018. Radford, A., Metz, L., and Chintala, S. Unsupervised Representa- tion Learning with Deep Convolutional Generative Adversarial Networks. arXiv e-prints, art. arXiv:1511.06434, Nov 2015. 3. Experiments and Discussion GAN Architecture The generator and discriminator are three layer (input-hidden-output) fully connected neural net- works with hidden dimension 64. We use Leaky ReLU(Xu et al., 2015) activations with 0.2 negative slope after the input and hidden layers. We use tanh for the generator out- put (dimensionality 28 × 28 = 784 to match MNIST) and sigmoid for the discriminator output (dimensionality 1). The generator samples from latent dimension 64. GAN-GAN Architecture The GAN-GAN generator and discriminator have the same layer and activation structure as the MNIST GAN. The input dimensionality is 113745, which is equal to the dimensionality of the GAN parameter vector. We found that using a smaller hidden dimension for the discriminator (8) than the generator (64) helped to stabilize training. We use latent dimension 1 for the GAN- GAN in order to enable visualizations. Results improve with a larger latent space. Training We use Adam(Kingma & Ba, 2014) for all net- works. The learning rate is ﬁxed to 0.0002; all other pa- rameters are PyTorch defaults. We use batch sizes 128 and 32 for MNIST and the GAN-GAN, respectively. As de- scribed in Algorithm 1, we train 35 MNIST GANs for 100 epochs each, saving snapshots of the weights at each epoch. We train the GAN-GAN for 250 epochs using these 3500 snapshots as training examples. Results In order to evaluate the performance of the GAN- GAN, we ﬁrst linearly sample 32 GANs from the 1- dimensional latent space of the GAN-GAN. We then ﬁx 40 noise samples with the same dimensionality as the GAN latent space. Finally, we sample 40 images from each GAN using the ﬁxed noise samples. Fig. 3 shows the results. The GAN-GAN produces neatly ordered GAN samples accord- ing to image quality. Surprisingly, the GAN-GAN actually exhibits better performance than the trained GANs. Given that high quality GANs appear more real than low quality GANs, it is possible that the GAN-GAN learned to bootstrap its own performance by implicitly combining snapshots (Liu et al., 2018) in latent space, in effect producing a smoothed history over training. Discussion While the GAN-GAN samples are of high quality with respect to the MNIST GAN samples, the MNIST GAN sam- ples themselves are of fairly low quality (Fig. 2). As the weight dimensionality of each GAN is equal to the input dimensionality of the GAN-GAN, we were constrained to use a very small network for the MNIST GAN. One obvious solution is to use a more parameter efﬁcient network archi- tecture such as DCGAN (Radford et al., 2015). While we Suarez, In Guyon, Language modeling with recurrent highway J. hypernetworks. I., Luxburg, U. V., Ben- gio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems 30, pp. 3267–3276. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 6919-language-modeling-with-recurrent-highway-hypernetworks. pdf. Xu, B., Wang, N., Chen, T., and Li, M. Empirical Evaluation of Rectiﬁed Activations in Convolutional Network. arXiv e-prints, art. arXiv:1505.00853, May 2015. Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired Image- to-Image Translation using Cycle-Consistent Adversarial Net- works. arXiv e-prints, art. arXiv:1703.10593, Mar 2017.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech(final_text = text):\n",
        "  tts = gTTS(final_text)\n",
        "  tts.save(paper.title+\".mp3\")\n",
        "\n",
        "  return(str(paper.title)+\".mp3\")"
      ],
      "metadata": {
        "id": "f6DeBDiiK1vE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_loc = text_to_speech(text)"
      ],
      "metadata": {
        "id": "lyHIzMkAWbTS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyttsx3;\n",
        "engine = pyttsx3.init();\n",
        "engine.say(\"I will speak this text\");"
      ],
      "metadata": {
        "id": "XJvpu_MyWieP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voices = engine.getProperty('voices')\n",
        "for voice in voices:\n",
        "   print(engine.setProperty('voice', voice.id))\n",
        "   engine.say('The quick brown fox jumped over the lazy dog.')\n",
        "engine.runAndWait()"
      ],
      "metadata": {
        "id": "J3PmD7YIZZVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc"
      ],
      "metadata": {
        "id": "aYbibiXKf-EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del engine \n",
        "gc.collect"
      ],
      "metadata": {
        "id": "jpndKV4McsoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pyttsx_(text=text):\n",
        "  engine = pyttsx3.init()\n",
        "  engine.setProperty(\"rate\", 178)\n",
        "  engine.setProperty('voice', voices[1].id)\n",
        "  engine.save_to_file(text , 'text.mp3')\n",
        "\n",
        "  engine.runAndWait()\n",
        "  del engine \n",
        "  gc.collect\n",
        "\n",
        "  return \"text.mp3\""
      ],
      "metadata": {
        "id": "a2cF6ioWYewX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_loc = pyttsx_(text)"
      ],
      "metadata": {
        "id": "pFerhmNTcOO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MIkM8FBbcbtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}