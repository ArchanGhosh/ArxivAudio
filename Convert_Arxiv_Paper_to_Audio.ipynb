{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Read and Convert PDF to Audio",
      "provenance": [],
      "authorship_tag": "ABX9TyPdBhnS+RPaUaMS7hlgw8s8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWeexOI1l0cg"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six[image]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "VrFCXuPWnEsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "8_AGaiW_xY-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfminer\n",
        "from pdfminer.high_level import extract_text, extract_pages"
      ],
      "metadata": {
        "id": "JtywpHmtrEXG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "search = arxiv.Search(id_list=[\"2203.09207\"])\n",
        "paper = next(search.results())\n",
        "print(paper.title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANJ3iBCSpA8E",
        "outputId": "e8246543-0f3e-4cd7-a894-02eb4f51fff7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper = next(arxiv.Search(id_list=[\"2203.09207\"]).results())\n",
        "# Download the PDF to the PWD with a default filename.\n",
        "paper.download_pdf(filename=(paper.title+\".pdf\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1RlXQxllpxOC",
        "outputId": "7ec4fc3e-c49c-4d97-a9f6-2d2ca3b7093a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text(paper.title+'.pdf')"
      ],
      "metadata": {
        "id": "M-_mx-WWp72f"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "o6g_4R4usI_K",
        "outputId": "94b1dc74-e620-4e4a-abe7-80744011c726"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2\\n2\\n0\\n2\\n \\nr\\na\\n\\nM\\n \\n7\\n1\\n \\n \\n]\\n\\nV\\n\\nI\\n.\\ns\\ns\\ne\\ne\\n[\\n \\n \\n1\\nv\\n7\\n0\\n2\\n9\\n0\\n.\\n3\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nSimulation-Driven Training of Vision\\nTransformers Enabling Metal Segmentation in\\nX-Ray Images\\n\\nFuxin Fan1, Ludwig Ritschl2, Marcel Beister2, Ramyar Biniazan2, Bj¨orn\\nKreher2, Tristan M. Gottschalk1,2, Steﬀen Kappler2, and Andreas Maier1\\n\\n1\\n\\nPattern Recognition Lab, Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg,\\nErlangen 91058, Germany\\nSiemens Healthcare GmbH, Forchheim, Germany\\n\\n2\\n\\nAbstract. In several image acquisition and processing steps of X-ray\\nradiography, knowledge of the existence of metal implants and their ex-\\nact position is highly beneﬁcial (e.g. dose regulation, image contrast ad-\\njustment). Another application which would beneﬁt from an accurate\\nmetal segmentation is cone beam computed tomography (CBCT) which\\nis based on 2D X-ray projections. Due to the high attenuation of metals,\\nsevere artifacts occur in the 3D X-ray acquisitions. The metal segmen-\\ntation in CBCT projections usually serves as a prerequisite for metal\\nartifact avoidance and reduction algorithms. Since the generation of high\\nquality clinical training is a constant challenge, this study proposes to\\ngenerate simulated X-ray images based on CT data sets combined with\\nself-designed computer aided design (CAD) implants and make use of\\nconvolutional neural network (CNN) and vision transformer (ViT) for\\nmetal segmentation. Model test is performed on accurately labeled X-\\nray test datasets obtained from specimen scans. The CNN encoder-based\\nnetwork like U-Net has limited performance on cadaver test data with an\\naverage dice score below 0.30, while the metal segmentation transformer\\nwith dual decoder (MST-DD) shows high robustness and generalization\\non the segmentation task, with an average dice score of 0.90. Our study\\nindicates that the CAD model-based data generation has high ﬂexibility\\nand could be a way to overcome the problem of shortage in clinical data\\nsampling and labelling. Furthermore, the MST-DD approach generates\\na more reliable neural network in case of training on simulated data.\\n\\nKeywords: CAD metal implants · Metal segmentation · Vision trans-\\nformer.\\n\\n1\\n\\nIntroduction\\n\\nMetallic implants are utilized for ﬁxation of trauma fractures during orthopaedic\\nsurgery. However, their existence inside the body poses a severe challenge for the\\ngeneration of X-ray images or acquiring ﬂuoroscopic image sequences for cone\\nbeam computed tomography (CBCT). On the one hand the existence of metal\\n\\n\\x0c2\\n\\nAuthors Suppressed Due to Excessive Length\\n\\ncomplicates automatic dose exposure during acquisition and must be treated\\nadequately during image processing in classical X-ray radiography. On the other\\nhand metal has strong negative impact on image quality in CBCT. Due to the\\nhigh attenuation of metallic objects, strong artifacts can appear which can limit\\nthe assessment of bone-implant integration and success of fusion procedures [1].\\nTo reduce the impact of metal artifacts, many algorithms have been developed,\\nsuch as metal artifact avoidance (MAA) and metal artifact reduction (MAR)\\nmethod. MAA tends to optimize scan geometry based on limited projections to\\nminimizes metal-induced biases in the projection data [2]. MAR focuses on the\\nremoval of artifacts [3]. Both methods rely on accurate segmentation of metals\\n[2–11] which is performed either directly on projections or on reconstructed\\nvolume. Most of the commercially available MAR algorithms are based on the\\nvolume based metal segmentation [3]. It is well known that detecting metal\\nobjects is problematic in CT volumes when they are outside of the scanner’s ﬁeld-\\nof-view (FOV). These limitations further motivate us to design a pure projection-\\nbased metal segmentation algorithm in this study.\\n\\nWith the development of deep learning approaches, neural network models\\nare widely used in image segmentation tasks and among diﬀerent model architec-\\ntures, U-Net [12] based models have been applied mostly in metal segmentation\\non projection domain such as the work by Hegazy et al. [4, 11]. In their study,\\nneural network models have been trained and tested on a ﬁnite quantity of clini-\\ncal and cadaver scans, which makes their algorithm highly dependent on training\\ndata and, therefore, the generalization of their model is not guaranteed.\\n\\nTraditionally training a segmentation algorithm for this task requires clinical\\nimage data consisting a combination of various types of metal implants, diﬀerent\\nbody regions and diﬀerent projection geometries. These data would need to un-\\ndergo a high quality annotation process which might still be inaccurate in case\\nof complex anatomical structures. In such scenarios, simulated projections have\\ngreat advantages because of their accurate annotations, variety projections and\\nthe possibility to utilize diﬀerent types of implants. In our case we combined\\nthe forward projection of CT data sets with the forward projection of metal vol-\\numes. Although simulation frameworks can provide a theoretical inﬁnite number\\nof training data, one should consider the diﬀerences between such data and real\\nclinical images. Therefore the DeepDRR framework [13] is used by considering\\nthe physical properties of clinical data. Furthermore, vision transformer (ViT)\\nmodels are well known to be able to transform between tasks [14]. Models such\\nas SETR [15], TransUNet [16], CoTr [17] have shown great potential to con-\\nstruct robust segmentation networks. This might be beneﬁcial for transferring\\nthe algorithms performance from simulated X-ray projection to real projection\\ndata.\\n\\nThe main contributions of this work can be summarized as follows:\\n\\n1) Computer-aided design (CAD) model based multi-metal projections gen-\\neration. This is the ﬁrst study to use CAD models to generate large numbers of\\nprojections for metallic implant segmentation.\\n\\n\\x0cTitle Suppressed Due to Excessive Length\\n\\n3\\n\\n2) Introduction of MST-DD, consisting of ViT encoder and dual connected\\n\\nCNN decoders. This is the ﬁrst model using ViT in metal segmentation task.\\n\\n2 Materials and Method\\n\\n2.1 Data generation\\n\\nThe pipeline for the generation of simulated projections is illustrated in Fig. 1.\\nThe CAD models of implants, such as K-wires, screws and curved plates with\\nholes are drawn using the software AutoCAD. Each of the models is transferred\\ninto a 3D binary image. 14 knee CT volumes have been selected from the SICAS\\nmedical image repository [18]. All volumes are rescaled to a voxel side length\\nof 0.5 mm and a volume size of 1000 × 600 × 600, among which 600 continues\\nslices are randomly chosen to merge with one multi-metal volume. To generate\\nthe multi-metal volume with the size of 600 × 600 × 600, implants are randomly\\nselected, rotated and translated. The Hounsﬁeld units (HU) value for each im-\\nplant is randomly chosen between 3000 and 8000. Because in reality implants\\ntypically have intersection with bone, one constraint for the translation is that\\nthe corresponding HU value at the location of the target knee volume is above\\n500 HU. Afterwards, the multi-metal volume is inserted into one target knee\\nvolume. In total, 50 volumes with diﬀerent metal distributions and anatomical\\nbackground are generated and forward projected afterwards.\\n\\nFig. 1. The pipeline of multi-metal projection generation\\n\\nMonochromatic and polyhromatic projections are simulated by CONRAD\\n[19] and DeepDRR framework [13], respectively. The distance between detector\\nand source is 1164 mm and the distance between detector and isocenter is 700\\nmm. The detector has the size of 976 × 976 and each pixel is 0.305 mm × 0.305\\n\\n\\x0c4\\n\\nAuthors Suppressed Due to Excessive Length\\n\\nmm. For one volume, 60 projections are generated with an increased rotation\\nangle of 6◦. For the simulation of polychromatic projection, the volume is de-\\ncomposed into 4 diﬀerent materials depending on their HU value: air (<800\\nHU), soft tissue ([-800, 350] HU), bone ((350, 2000] HU) and metallic implants\\n(>2000 HU). After Poisson noise injection and log transform, all projections are\\nnormalized. Furthermore, we use the gamma function for brightness adjustment\\nas data augmentation. The gamma value is randomly chosen between 0.7 and\\n1.3. For monochromatic and polychromatic dataset, 50 sets of projections are\\nsimulated for each dataset. 45 sets among them are used for training and 5 sets\\nare for validation.\\n\\nAs test dataset we use real X-ray projection data taken from an experimental\\ncadaver study. The cadaver scans were acquired using a mobile C-arm system\\nwith CBCT capability. All scans were acquired with and without metallic im-\\nplants while keeping the scan setup in a constant position. This helps to generate\\nhighly accurate metal annotations by subtracting two corresponding scans. 10\\nscans are used as test data set while each scan includes 400 projections.\\n\\n2.2 Neural networks\\n\\nThe neural networks used in this work are demonstrated in Fig. 2. The basic\\nnetwork modules are classiﬁed into four categories: contraction module, convo-\\nlutional module, expansion module and transformer module. The contraction\\nmodule in this work includes four blocks and every block consists of two se-\\nquential convolutional layers, each of which is followed with a ReLU layer and\\na batch normalization (BN) layer. Finally, the output of these layers goes to\\na max pooling layer. After that the ﬁlter size is doubled for convolutional lay-\\ners in next block. The convolutional module and the expansion module have\\none and four blocks respectively, both of which have similar structures as the\\ncontraction block, with the diﬀerence that there is no pooling layer in the convo-\\nlutional block and the expansion block. Moreover, the upsampling layer is added\\nin the beginning of the ﬁrst convolutional layer of the expansion block. The ﬁlter\\nsize for the convolutional block is doubled as its previous contraction block in\\nU-Net (Fig. 2(a)), while the ﬁlter size is halved in the expansion blocks after\\neach upsampling layer. In this work, the transformer module used in TransUNet\\n(Fig. 2(b)) and metal segmentation transformer with single decoder (MST-SD)\\n(Fig. 2(c)) has 12 transformer blocks, each of which starts with a layer nor-\\nmalization (LN), followed by a multi-headed self-attention layer, another LN\\nand ﬁnally one fully connected layer (multi-layer perception). The input before\\neach LN will be summed with the output of MSA and MLP, respectively. For\\nMST with dual decoder (MST-DD) (Fig. 2(d)), both transformer modules have\\n6 blocks, and they are followed by two expansion modules with skip connections.\\nIn the end, Sigmoid function is used as the active function for the output of the\\nall networks.\\n\\nIn this work, the input image has the size of 512 × 512, which is a patch\\nrandomly sampled from the original image. The further hyperparameters for the\\nnetworks are as follows The ﬁlter size of the ﬁrst contraction block and the last\\n\\n\\x0cTitle Suppressed Due to Excessive Length\\n\\n5\\n\\nFig. 2. Four Neural networks structures. a) U-Net. b) TransUNet. c) MST-SD. d) MST-\\nDD. They are constructed by diﬀerent modules which consist of several corresponding\\nblocks.\\n\\nexpansion block is 64. The stride size for maxpooling and upsampling is 2. The\\npositional encoding for ViT is from 1 to the sequence size. The patch size for\\nsplitting the input image is 16 × 16 and each patch is ﬂattened and embedded\\ninto 256. There are four heads for the multi-headed self attention layer. The ﬁlter\\nsize is 512 for the ﬁrst expansion block in TransUNet, MST-SD and MST-DD.\\nAll the networks are trained for maximum 50 epochs. The default loss func-\\ntion is binary cross entropy while dice loss is incorporated for comparison. The\\ntotal number of training and validation projections for one epoch is 2700 and\\n300, respectively. For each projection, 5 patches are sampled for training and\\nvalidation. We used Adam optimizer with an initial learning rate of 0.001 and\\nan exponential decay of 0.95 after one epoch. The evaluation metrics are dice\\nscore, precision and recall for cadaver test data.\\n\\n3 Results\\n\\nThe evaluation results are summarized in Tab. 1 and Tab. 2. All four networks\\nare trained on monochromatic and polychromatic projections separately. For the\\n\\n\\x0c6\\n\\nAuthors Suppressed Due to Excessive Length\\n\\nmodels trained on monochromatic projections, both U-Net and TransUNet have\\npoor performance with average dice score lower than 0.30 and average precision\\nlower than 0.20. MST-SD model generates slightly better predictions, but the\\naverage dice score is still below 0.70 with a high variance of 0.16. By using\\ndual decoder, the average dice score will increase to a value above 0.85 with\\na variance of 0.09 and, ﬁnally, the change of loss function to dice makes the\\nMST-DD achieve 0.90 average dice score and a variance of 0.03.\\n\\nTable 1. Evaluation results for networks trained by monochromatic projections.\\n\\nTest\\n\\nCadaver\\n\\nAvg. Dice\\n0.21 ± 0.12\\nU-Net\\n0.28 ± 0.13\\nTranUNet\\n0.69 ± 0.16\\nMST-SD\\n0.86 ± 0.09\\nMST-DD\\nMST-DD (Dice Loss) 0.90 ± 0.03\\n\\nAvg. Precision Avg. Recall\\n0.97 ± 0.02\\n0.12 ± 0.09\\n0.93 ± 0.04\\n0.17 ± 0.09\\n0.81 ± 0.10\\n0.64 ± 0.22\\n0.80 ± 0.13\\n0.95 ± 0.01\\n0.92 ± 0.07\\n0.88 ± 0.02\\n\\nFor model trained on polychromatic projections, the average dice score for\\nthe U-Net and TransUNet is below 0.35. MST-SD has higher average dice score\\nof 0.76. MST-DD models have the same dice score of 0.89, but with dice loss the\\nvariance for dice score is 0.02 lower.\\n\\nTable 2. Evaluation results for networks trained by polychromatic projections.\\n\\nTest\\n\\nCadaver\\n\\nAvg. Dice\\n0.27 ± 0.14\\nU-Net\\n0.33 ± 0.18\\nTranUNet\\n0.76 ± 0.15\\nMST-SD\\n0.89 ± 0.07\\nMST-DD\\nMST-DD (Dice Loss) 0.89 ± 0.05\\n\\nAvg. Precision Avg. Recall\\n0.88 ± 0.08\\n0.17 ± 0.10\\n0.95 ± 0.03\\n0.22 ± 0.15\\n0.66 ± 0.19\\n0.95 ± 0.01\\n0.85 ± 0.11\\n0.94 ± 0.02\\n0.90 ± 0.09\\n0.89 ± 0.02\\n\\nThe predictions for cadaver projections are displayed in Fig. 3. It shows\\nfour knee scans with diﬀerent implants (Fig. 3(a1-d1)) and one spine scan with\\nscrews (Fig. 3(e1)). All models are trained on polychromatic projections. The\\nintersected areas of predictions and labels are colored in white. The red and\\ngreen areas stand for the false positives and false negatives, respectively. As it\\ncan be seen, The U-Net and TransUNet predict a large range of false positives\\nnear the bones. On the other hand, predictions of MST-SD have almost no false\\npositives but still show incomplete segmentation of implants. By using the MST-\\nDD and MST-DD with dice loss the performance is improved, with little false\\npositives near the metal and less missing areas compared to MST-SD.\\n\\n\\x0cTitle Suppressed Due to Excessive Length\\n\\n7\\n\\nFig. 3. Prediction results for real cadaver scans from diﬀerent neural networks trained\\non simulated polychromatic projections. The true positive segmentation is labeled in\\nwhite. The false positive and false negative segmentation are labeled in red and green,\\nrespectively.\\n\\n\\x0c8\\n\\nAuthors Suppressed Due to Excessive Length\\n\\n4 Discussion\\n\\nLeveraging the high ﬂexibility of metal simulation and insertion into real CT\\ndata enables us to generate large quantities of projection data. This can over-\\ncome the problem of data shortage for deep learning in the ﬁeld of medical\\nimage processing. Another highly important aspect which supports the idea of\\nsimulated data is the high accuracy of segmentation labels.\\n\\nThe ViT started to draw attention since 2020. One important advantage is\\nthat it can transform between diﬀerent tasks at the expense of requiring a large\\nquantity of data for the initial training. After replacing the encoder from CNN\\nto ViT, the performance of our setup greatly improved using the same training\\ndata, which can be testiﬁed by the results of MST-SD in Tab. 1,2 and Fig. 3.\\nFurther improvement is made after adding one more decoder, demonstrated by\\nan average dice score of MST-DD being above 0.85. The implementation of dice\\nloss helps to improve the average recall in general and reduce the deviation for\\nthe mean dice score of MST-DD.\\n\\nEvaluating the performance of networks trained on monochromatic and poly-\\nchromatic projections, we can observe very limited improvement using polychro-\\nmatic projections for U-Net and TransUNet. For the proposed MST-DD with\\ndice loss, the segmentation performance is comparable for both data sets. ViT\\nbased encoder has less sensitivity and can focus on extracting the right features.\\nA limitation of this study is the reduction to the knee area as anatomical\\nbackground. This could be generalized to the whole body in future work to make\\nthe network more robust. Another direction of research could be the fusion of\\nsimulated metal with real high resolution X-ray images instead of using synthetic\\nimages generated from CT volumes.\\n\\n5 Conclusion\\n\\nThis work proposes the use of simulated training data combining CAD-based\\nimplants and CT volumes containing human anatomy. Aditionally we propose\\na dedicated network design MST-DD for metal segmentation. Combining the\\npower of the large quantity of simulated images and the design of the ViT encoder\\nand dual connected decoder enable us to construct a robust neural network which\\nperforms on real X-ray cadaver projections with an average dice score of 0.90.\\n\\nThis work was supported by academic-industrial collaboration with Siemens\\nHealthineers, XP Division. The presented method is not commercially available.\\n\\n6 Disclaimer\\n\\nReferences\\n\\n1. Netto, C., Mansur, N., Tazegul, T., Lalevee, M., Lee, H., Behrens, A., Lintz, F.,\\nGodoy-Santos, A., Dibbern, K., Anderson, D. Implant Related Artifact Around\\n\\n\\x0cTitle Suppressed Due to Excessive Length\\n\\n9\\n\\nMetallic and Bio-Integrative Screws: A CT Scan 3D Hounsﬁeld Unit Assessment.\\nFoot & Ankle Orthopaedics. 7, 2473011421S00174 (2022)\\n\\n2. Wu P, Sheth N, Sisniega A, Uneri A, Han R, Vijayan R, Vagdargi P, Kreher B,\\nKunze H, Kleinszig G, Vogt S. C-arm orbits for metal artifact avoidance (MAA) in\\ncone-beam CT. Physics In Medicine & Biology. 65, 165012 (2020)\\n\\n3. Katsura, M., Sato, J., Akahane, M., Kunimatsu, A., Abe, O. Current and novel\\ntechniques for metal artifact reduction at CT: practical guide for radiologists. Ra-\\ndiographics. 38, 450-461 (2018)\\n\\n4. Hegazy, M., Cho, M., Cho, M., Lee, S. U-net based metal segmentation on projection\\ndomain for metal artifact reduction in dental CT. Biomedical Engineering Letters.\\n9, 375-385 (2019)\\n\\n5. Shi, L., Bennett, N., Shiroma, A., Sun, M., Zhang, J., Colbeth, R., Star-Lack, J.,\\nLu, M., Wang, A. Single-pass metal artifact reduction using a dual-layer ﬂat panel\\ndetector. Medical Physics. 48, 6482-6496 (2021)\\n\\n6. Zhang, Y., Zhang, L., Zhu, X., Lee, A., Chambers, M., Dong, L. Reducing metal\\nartifacts in cone-beam CT images by preprocessing projection data. International\\nJournal Of Radiation Oncology* Biology* Physics. 67, 924-932 (2007)\\n\\n7. Bazalova, M., Beaulieu, L., Palefsky, S., Verhaegen, F. Correction of CT artifacts\\nand its inﬂuence on Monte Carlo dose calculations. Medical Physics. 34, 2119-2132\\n(2007)\\n\\n8. Li, H., Yu, L., Liu, X., Fletcher, J., McCollough, C. Metal artifact suppression from\\nreformatted projections in multislice helical CT using dual-front active contours.\\nMedical Physics. 37, 5155-5164 (2010)\\n\\n9. Yu, L., Li, H., Mueller, J., Koﬂer, J., Liu, X., Primak, A., Fletcher, J., Guimaraes, L.,\\nMacedo, T., McCollough, C. Metal artifact reduction from reformatted projections\\nfor hip prostheses in multislice helical computed tomography: techniques and initial\\nclinical results. Investigative Radiology. 44, 691 (2009)\\n\\n10. Yu, H., Zeng, K., Bharkhada, D., Wang, G., Madsen, M., Saba, O., Policeni, B.,\\nHoward, M., Smoker, W. A segmentation-based method for metal artifact reduction.\\nAcademic Radiology. 14, 495-504 (2007)\\n\\n11. Gottschalk, T., Maier, A., Kordon, F., Kreher, B. View-Consistent Metal Seg-\\nmentation in the Projection Domain for Metal Artifact Reduction in CBCT – An\\nInvestigation of Potential Improvement. Machine Learning For Biomedical Imaging.\\n1 (2021)\\n\\n12. Ronneberger, O., Fischer, P., Brox, T. U-net: Convolutional networks for biomed-\\nical image segmentation. International Conference On Medical Image Computing\\nAnd Computer-assisted Intervention. pp. 234-241 (2015)\\n\\n13. Unberath, M., Zaech, J., Lee, S., Bier, B., Fotouhi, J., Armand, M., Navab, N.\\nDeepdrr–a catalyst for machine learning in ﬂuoroscopy-guided procedures. Interna-\\ntional Conference On Medical Image Computing And Computer-Assisted Interven-\\ntion. pp. 98-106 (2018)\\n\\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-\\nterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J.,\\nHoulsby, N. An Image is Worth 16x16 Words: Transformers for Image Recog-\\nnition at Scale. International Conference On Learning Representations. (2021),\\nhttps://openreview.net/forum?id=YicbFdNTTy\\n\\n15. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang,\\nT., Torr, P.H. and Zhang, L. Rethinking semantic segmentation from a sequence-to-\\nsequence perspective with transformers. Proceedings Of The IEEE/CVF Conference\\nOn Computer Vision And Pattern Recognition. pp. 6881-6890 (2021)\\n\\n\\x0c10\\n\\nAuthors Suppressed Due to Excessive Length\\n\\n16. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A., Zhou,\\nY. Transunet: Transformers make strong encoders for medical image segmentation.\\nArXiv Preprint arXiv:2102.04306. (2021)\\n\\n17. Xie, Y., Zhang, J., Shen, C., Xia, Y. Cotr: Eﬃciently bridging cnn and transformer\\nfor 3d medical image segmentation. International Conference On Medical Image\\nComputing And Computer-assisted Intervention. pp. 171-180 (2021)\\n\\n18. Kistler, M., Bonaretti, S., Pfahrer, M., Niklaus, R., B¨uchler, P. The virtual skele-\\nton database: an open access repository for biomedical research and collaboration.\\nJournal Of Medical Internet Research. 15, e2930 (2013)\\n\\n19. Maier, A., Hofmann, H.G., Berger, M., Fischer, P., Schwemmer, C., Wu, H., M¨uller,\\nK., Hornegger, J., Choi, J.H., Riess, C. and Keil, A. CONRAD—A software frame-\\nwork for cone-beam imaging in radiology. Medical Physics. 40, 111914 (2013)\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS"
      ],
      "metadata": {
        "id": "1tXGcoEBw2RQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tts = gTTS(text)\n",
        "tts.save(paper.title+\".mp3\")"
      ],
      "metadata": {
        "id": "UFi0ot5CxfOn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textract"
      ],
      "metadata": {
        "id": "-zyByHK31Zi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text = textract.process(paper.title+'.pdf', method='pdfminer', encoding='utf-8')\n",
        "#text = text.encode(\"utf-8\")\n",
        "text"
      ],
      "metadata": {
        "id": "P-9w7zXt2De1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dec = text.decode('utf-8')\n",
        "print(text_dec)"
      ],
      "metadata": {
        "id": "Gve73JHp51k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = text_dec.splitlines()\n",
        "test"
      ],
      "metadata": {
        "id": "Wvox1zgT6I1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = []\n",
        "final_text=\"\"\n",
        "for i in test:\n",
        "  if len(i)>1:\n",
        "    final.append(i)\n",
        "    final_text+= \" \"+i\n",
        "\n",
        "final"
      ],
      "metadata": {
        "id": "AC7v76oQ3HpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "oEClm9pl7gB0",
        "outputId": "217de1bb-44ae-4d65-d605-73e871c1bca2"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images Fuxin Fan1, Ludwig Ritschl2, Marcel Beister2, Ramyar Biniazan2, Bj¨orn Kreher2, Tristan M. Gottschalk1 2, Steﬀen Kappler2, and Andreas Maier1 1 Pattern Recognition Lab, Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg, Erlangen 91058, Germany 2 Siemens Healthcare GmbH, Forchheim, Germany Abstract. In several image acquisition and processing steps of X-ray radiography, knowledge of the existence of metal implants and their ex- act position is highly beneﬁcial (e.g. dose regulation, image contrast ad- justment). Another application which would beneﬁt from an accurate metal segmentation is cone beam computed tomography (CBCT) which is based on 2D X-ray projections. Due to the high attenuation of metals, severe artifacts occur in the 3D X-ray acquisitions. The metal segmen- tation in CBCT projections usually serves as a prerequisite for metal artifact avoidance and reduction algorithms. Since the generation of high quality clinical training is a constant challenge, this study proposes to generate simulated X-ray images based on CT data sets combined with self-designed computer aided design (CAD) implants and make use of convolutional neural network (CNN) and vision transformer (ViT) for metal segmentation. Model test is performed on accurately labeled X- ray test datasets obtained from specimen scans. The CNN encoder-based network like U-Net has limited performance on cadaver test data with an average dice score below 0.30, while the metal segmentation transformer with dual decoder (MST-DD) shows high robustness and generalization on the segmentation task, with an average dice score of 0.90. Our study indicates that the CAD model-based data generation has high ﬂexibility and could be a way to overcome the problem of shortage in clinical data sampling and labelling. Furthermore, the MST-DD approach generates a more reliable neural network in case of training on simulated data. Keywords: CAD metal implants · Metal segmentation · Vision trans- former. Introduction Metallic implants are utilized for ﬁxation of trauma fractures during orthopaedic surgery. However, their existence inside the body poses a severe challenge for the generation of X-ray images or acquiring ﬂuoroscopic image sequences for cone beam computed tomography (CBCT). On the one hand the existence of metal Authors Suppressed Due to Excessive Length complicates automatic dose exposure during acquisition and must be treated adequately during image processing in classical X-ray radiography. On the other hand metal has strong negative impact on image quality in CBCT. Due to the high attenuation of metallic objects, strong artifacts can appear which can limit the assessment of bone-implant integration and success of fusion procedures [1]. To reduce the impact of metal artifacts, many algorithms have been developed, such as metal artifact avoidance (MAA) and metal artifact reduction (MAR) method. MAA tends to optimize scan geometry based on limited projections to minimizes metal-induced biases in the projection data [2]. MAR focuses on the removal of artifacts [3]. Both methods rely on accurate segmentation of metals [2–11] which is performed either directly on projections or on reconstructed volume. Most of the commercially available MAR algorithms are based on the volume based metal segmentation [3]. It is well known that detecting metal objects is problematic in CT volumes when they are outside of the scanner’s ﬁeld- of-view (FOV). These limitations further motivate us to design a pure projection- based metal segmentation algorithm in this study. With the development of deep learning approaches, neural network models are widely used in image segmentation tasks and among diﬀerent model architec- tures, U-Net [12] based models have been applied mostly in metal segmentation on projection domain such as the work by Hegazy et al. [4, 11]. In their study, neural network models have been trained and tested on a ﬁnite quantity of clini- cal and cadaver scans, which makes their algorithm highly dependent on training data and, therefore, the generalization of their model is not guaranteed. Traditionally training a segmentation algorithm for this task requires clinical image data consisting a combination of various types of metal implants, diﬀerent body regions and diﬀerent projection geometries. These data would need to un- dergo a high quality annotation process which might still be inaccurate in case of complex anatomical structures. In such scenarios, simulated projections have great advantages because of their accurate annotations, variety projections and the possibility to utilize diﬀerent types of implants. In our case we combined the forward projection of CT data sets with the forward projection of metal vol- umes. Although simulation frameworks can provide a theoretical inﬁnite number of training data, one should consider the diﬀerences between such data and real clinical images. Therefore the DeepDRR framework [13] is used by considering the physical properties of clinical data. Furthermore, vision transformer (ViT) models are well known to be able to transform between tasks [14]. Models such as SETR [15], TransUNet [16], CoTr [17] have shown great potential to con- struct robust segmentation networks. This might be beneﬁcial for transferring the algorithms performance from simulated X-ray projection to real projection data. The main contributions of this work can be summarized as follows: 1) Computer-aided design (CAD) model based multi-metal projections gen- eration. This is the ﬁrst study to use CAD models to generate large numbers of projections for metallic implant segmentation. Title Suppressed Due to Excessive Length 2) Introduction of MST-DD, consisting of ViT encoder and dual connected CNN decoders. This is the ﬁrst model using ViT in metal segmentation task. 2 Materials and Method 2.1 Data generation The pipeline for the generation of simulated projections is illustrated in Fig. 1. The CAD models of implants, such as K-wires, screws and curved plates with holes are drawn using the software AutoCAD. Each of the models is transferred into a 3D binary image. 14 knee CT volumes have been selected from the SICAS medical image repository [18]. All volumes are rescaled to a voxel side length of 0.5 mm and a volume size of 1000 × 600 × 600, among which 600 continues slices are randomly chosen to merge with one multi-metal volume. To generate the multi-metal volume with the size of 600 × 600 × 600, implants are randomly selected, rotated and translated. The Hounsﬁeld units (HU) value for each im- plant is randomly chosen between 3000 and 8000. Because in reality implants typically have intersection with bone, one constraint for the translation is that the corresponding HU value at the location of the target knee volume is above 500 HU. Afterwards, the multi-metal volume is inserted into one target knee volume. In total, 50 volumes with diﬀerent metal distributions and anatomical background are generated and forward projected afterwards. Fig. 1. The pipeline of multi-metal projection generation Monochromatic and polyhromatic projections are simulated by CONRAD [19] and DeepDRR framework [13], respectively. The distance between detector and source is 1164 mm and the distance between detector and isocenter is 700 mm. The detector has the size of 976 × 976 and each pixel is 0.305 mm × 0.305 Authors Suppressed Due to Excessive Length mm. For one volume, 60 projections are generated with an increased rotation angle of 6◦. For the simulation of polychromatic projection, the volume is de- composed into 4 diﬀerent materials depending on their HU value: air (<800 HU), soft tissue ([-800, 350] HU), bone ((350, 2000] HU) and metallic implants (>2000 HU). After Poisson noise injection and log transform, all projections are normalized. Furthermore, we use the gamma function for brightness adjustment as data augmentation. The gamma value is randomly chosen between 0.7 and 1.3. For monochromatic and polychromatic dataset, 50 sets of projections are simulated for each dataset. 45 sets among them are used for training and 5 sets are for validation. As test dataset we use real X-ray projection data taken from an experimental cadaver study. The cadaver scans were acquired using a mobile C-arm system with CBCT capability. All scans were acquired with and without metallic im- plants while keeping the scan setup in a constant position. This helps to generate highly accurate metal annotations by subtracting two corresponding scans. 10 scans are used as test data set while each scan includes 400 projections. 2.2 Neural networks The neural networks used in this work are demonstrated in Fig. 2. The basic network modules are classiﬁed into four categories: contraction module, convo- lutional module, expansion module and transformer module. The contraction module in this work includes four blocks and every block consists of two se- quential convolutional layers, each of which is followed with a ReLU layer and a batch normalization (BN) layer. Finally, the output of these layers goes to a max pooling layer. After that the ﬁlter size is doubled for convolutional lay- ers in next block. The convolutional module and the expansion module have one and four blocks respectively, both of which have similar structures as the contraction block, with the diﬀerence that there is no pooling layer in the convo- lutional block and the expansion block. Moreover, the upsampling layer is added in the beginning of the ﬁrst convolutional layer of the expansion block. The ﬁlter size for the convolutional block is doubled as its previous contraction block in U-Net (Fig. 2(a)), while the ﬁlter size is halved in the expansion blocks after each upsampling layer. In this work, the transformer module used in TransUNet (Fig. 2(b)) and metal segmentation transformer with single decoder (MST-SD) (Fig. 2(c)) has 12 transformer blocks, each of which starts with a layer nor- malization (LN), followed by a multi-headed self-attention layer, another LN and ﬁnally one fully connected layer (multi-layer perception). The input before each LN will be summed with the output of MSA and MLP, respectively. For MST with dual decoder (MST-DD) (Fig. 2(d)), both transformer modules have 6 blocks, and they are followed by two expansion modules with skip connections. In the end, Sigmoid function is used as the active function for the output of the all networks. In this work, the input image has the size of 512 × 512, which is a patch randomly sampled from the original image. The further hyperparameters for the networks are as follows The ﬁlter size of the ﬁrst contraction block and the last Title Suppressed Due to Excessive Length Fig. 2. Four Neural networks structures. a) U-Net. b) TransUNet. c) MST-SD. d) MST- DD. They are constructed by diﬀerent modules which consist of several corresponding blocks. expansion block is 64. The stride size for maxpooling and upsampling is 2. The positional encoding for ViT is from 1 to the sequence size. The patch size for splitting the input image is 16 × 16 and each patch is ﬂattened and embedded into 256. There are four heads for the multi-headed self attention layer. The ﬁlter size is 512 for the ﬁrst expansion block in TransUNet, MST-SD and MST-DD. All the networks are trained for maximum 50 epochs. The default loss func- tion is binary cross entropy while dice loss is incorporated for comparison. The total number of training and validation projections for one epoch is 2700 and 300, respectively. For each projection, 5 patches are sampled for training and validation. We used Adam optimizer with an initial learning rate of 0.001 and an exponential decay of 0.95 after one epoch. The evaluation metrics are dice score, precision and recall for cadaver test data. 3 Results The evaluation results are summarized in Tab. 1 and Tab. 2. All four networks are trained on monochromatic and polychromatic projections separately. For the Authors Suppressed Due to Excessive Length models trained on monochromatic projections, both U-Net and TransUNet have poor performance with average dice score lower than 0.30 and average precision lower than 0.20. MST-SD model generates slightly better predictions, but the average dice score is still below 0.70 with a high variance of 0.16. By using dual decoder, the average dice score will increase to a value above 0.85 with a variance of 0.09 and, ﬁnally, the change of loss function to dice makes the MST-DD achieve 0.90 average dice score and a variance of 0.03. Table 1. Evaluation results for networks trained by monochromatic projections. Test Cadaver Avg. Dice 0.21 ± 0.12 U-Net 0.28 ± 0.13 TranUNet 0.69 ± 0.16 MST-SD MST-DD 0.86 ± 0.09 MST-DD (Dice Loss) 0.90 ± 0.03 Avg. Precision Avg. Recall 0.97 ± 0.02 0.12 ± 0.09 0.17 ± 0.09 0.93 ± 0.04 0.81 ± 0.10 0.64 ± 0.22 0.80 ± 0.13 0.95 ± 0.01 0.88 ± 0.02 0.92 ± 0.07 For model trained on polychromatic projections, the average dice score for the U-Net and TransUNet is below 0.35. MST-SD has higher average dice score of 0.76. MST-DD models have the same dice score of 0.89, but with dice loss the variance for dice score is 0.02 lower. Table 2. Evaluation results for networks trained by polychromatic projections. Test Cadaver Avg. Dice 0.27 ± 0.14 U-Net 0.33 ± 0.18 TranUNet 0.76 ± 0.15 MST-SD MST-DD 0.89 ± 0.07 MST-DD (Dice Loss) 0.89 ± 0.05 Avg. Precision Avg. Recall 0.88 ± 0.08 0.17 ± 0.10 0.95 ± 0.03 0.22 ± 0.15 0.95 ± 0.01 0.66 ± 0.19 0.85 ± 0.11 0.94 ± 0.02 0.89 ± 0.02 0.90 ± 0.09 The predictions for cadaver projections are displayed in Fig. 3. It shows four knee scans with diﬀerent implants (Fig. 3(a1-d1)) and one spine scan with screws (Fig. 3(e1)). All models are trained on polychromatic projections. The intersected areas of predictions and labels are colored in white. The red and green areas stand for the false positives and false negatives, respectively. As it can be seen, The U-Net and TransUNet predict a large range of false positives near the bones. On the other hand, predictions of MST-SD have almost no false positives but still show incomplete segmentation of implants. By using the MST- DD and MST-DD with dice loss the performance is improved, with little false positives near the metal and less missing areas compared to MST-SD. Title Suppressed Due to Excessive Length Fig. 3. Prediction results for real cadaver scans from diﬀerent neural networks trained on simulated polychromatic projections. The true positive segmentation is labeled in white. The false positive and false negative segmentation are labeled in red and green, respectively. Authors Suppressed Due to Excessive Length 4 Discussion Leveraging the high ﬂexibility of metal simulation and insertion into real CT data enables us to generate large quantities of projection data. This can over- come the problem of data shortage for deep learning in the ﬁeld of medical image processing. Another highly important aspect which supports the idea of simulated data is the high accuracy of segmentation labels. The ViT started to draw attention since 2020. One important advantage is that it can transform between diﬀerent tasks at the expense of requiring a large quantity of data for the initial training. After replacing the encoder from CNN to ViT, the performance of our setup greatly improved using the same training data, which can be testiﬁed by the results of MST-SD in Tab. 1,2 and Fig. 3. Further improvement is made after adding one more decoder, demonstrated by an average dice score of MST-DD being above 0.85. The implementation of dice loss helps to improve the average recall in general and reduce the deviation for the mean dice score of MST-DD. Evaluating the performance of networks trained on monochromatic and poly- chromatic projections, we can observe very limited improvement using polychro- matic projections for U-Net and TransUNet. For the proposed MST-DD with dice loss, the segmentation performance is comparable for both data sets. ViT based encoder has less sensitivity and can focus on extracting the right features. A limitation of this study is the reduction to the knee area as anatomical background. This could be generalized to the whole body in future work to make the network more robust. Another direction of research could be the fusion of simulated metal with real high resolution X-ray images instead of using synthetic images generated from CT volumes. 5 Conclusion This work proposes the use of simulated training data combining CAD-based implants and CT volumes containing human anatomy. Aditionally we propose a dedicated network design MST-DD for metal segmentation. Combining the power of the large quantity of simulated images and the design of the ViT encoder and dual connected decoder enable us to construct a robust neural network which performs on real X-ray cadaver projections with an average dice score of 0.90. This work was supported by academic-industrial collaboration with Siemens Healthineers, XP Division. The presented method is not commercially available. 6 Disclaimer References 1. Netto, C., Mansur, N., Tazegul, T., Lalevee, M., Lee, H., Behrens, A., Lintz, F., Godoy-Santos, A., Dibbern, K., Anderson, D. Implant Related Artifact Around Title Suppressed Due to Excessive Length Metallic and Bio-Integrative Screws: A CT Scan 3D Hounsﬁeld Unit Assessment. Foot & Ankle Orthopaedics. 7, 2473011421S00174 (2022) 2. Wu P, Sheth N, Sisniega A, Uneri A, Han R, Vijayan R, Vagdargi P, Kreher B, Kunze H, Kleinszig G, Vogt S. C-arm orbits for metal artifact avoidance (MAA) in cone-beam CT. Physics In Medicine & Biology. 65, 165012 (2020) 3. Katsura, M., Sato, J., Akahane, M., Kunimatsu, A., Abe, O. Current and novel techniques for metal artifact reduction at CT: practical guide for radiologists. Ra- diographics. 38, 450-461 (2018) 4. Hegazy, M., Cho, M., Cho, M., Lee, S. U-net based metal segmentation on projection domain for metal artifact reduction in dental CT. Biomedical Engineering Letters. 9, 375-385 (2019) 5. Shi, L., Bennett, N., Shiroma, A., Sun, M., Zhang, J., Colbeth, R., Star-Lack, J., Lu, M., Wang, A. Single-pass metal artifact reduction using a dual-layer ﬂat panel detector. Medical Physics. 48, 6482-6496 (2021) 6. Zhang, Y., Zhang, L., Zhu, X., Lee, A., Chambers, M., Dong, L. Reducing metal artifacts in cone-beam CT images by preprocessing projection data. International Journal Of Radiation Oncology* Biology* Physics. 67, 924-932 (2007) 7. Bazalova, M., Beaulieu, L., Palefsky, S., Verhaegen, F. Correction of CT artifacts and its inﬂuence on Monte Carlo dose calculations. Medical Physics. 34, 2119-2132 (2007) 8. Li, H., Yu, L., Liu, X., Fletcher, J., McCollough, C. Metal artifact suppression from reformatted projections in multislice helical CT using dual-front active contours. Medical Physics. 37, 5155-5164 (2010) 9. Yu, L., Li, H., Mueller, J., Koﬂer, J., Liu, X., Primak, A., Fletcher, J., Guimaraes, L., Macedo, T., McCollough, C. Metal artifact reduction from reformatted projections for hip prostheses in multislice helical computed tomography: techniques and initial clinical results. Investigative Radiology. 44, 691 (2009) 10. Yu, H., Zeng, K., Bharkhada, D., Wang, G., Madsen, M., Saba, O., Policeni, B., Howard, M., Smoker, W. A segmentation-based method for metal artifact reduction. Academic Radiology. 14, 495-504 (2007) 11. Gottschalk, T., Maier, A., Kordon, F., Kreher, B. View-Consistent Metal Seg- mentation in the Projection Domain for Metal Artifact Reduction in CBCT – An Investigation of Potential Improvement. Machine Learning For Biomedical Imaging. 1 (2021) 12. Ronneberger, O., Fischer, P., Brox, T. U-net: Convolutional networks for biomed- ical image segmentation. International Conference On Medical Image Computing And Computer-assisted Intervention. pp. 234-241 (2015) 13. Unberath, M., Zaech, J., Lee, S., Bier, B., Fotouhi, J., Armand, M., Navab, N. Deepdrr–a catalyst for machine learning in ﬂuoroscopy-guided procedures. Interna- tional Conference On Medical Image Computing And Computer-Assisted Interven- tion. pp. 98-106 (2018) 14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un- terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recog- nition at Scale. International Conference On Learning Representations. (2021), https://openreview.net/forum?id=YicbFdNTTy 15. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P.H. and Zhang, L. Rethinking semantic segmentation from a sequence-to- sequence perspective with transformers. Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition. pp. 6881-6890 (2021) 10 Authors Suppressed Due to Excessive Length 16. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A., Zhou, Y. Transunet: Transformers make strong encoders for medical image segmentation. ArXiv Preprint arXiv:2102.04306. (2021) 17. Xie, Y., Zhang, J., Shen, C., Xia, Y. Cotr: Eﬃciently bridging cnn and transformer for 3d medical image segmentation. International Conference On Medical Image Computing And Computer-assisted Intervention. pp. 171-180 (2021) 18. Kistler, M., Bonaretti, S., Pfahrer, M., Niklaus, R., B¨uchler, P. The virtual skele- ton database: an open access repository for biomedical research and collaboration. Journal Of Medical Internet Research. 15, e2930 (2013) 19. Maier, A., Hofmann, H.G., Berger, M., Fischer, P., Schwemmer, C., Wu, H., M¨uller, K., Hornegger, J., Choi, J.H., Riess, C. and Keil, A. CONRAD—A software frame- work for cone-beam imaging in radiology. Medical Physics. 40, 111914 (2013)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tts = gTTS(final_text)\n",
        "tts.save(paper.title+\".mp3\")"
      ],
      "metadata": {
        "id": "n037ec0f8LHg"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tbj6XlC48fg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}